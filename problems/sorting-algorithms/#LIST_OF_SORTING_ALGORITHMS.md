# üîÄ List Of Sorting Algorithms

Sorting algorithms are techniques used to arrange data in a specific order (typically ascending or descending). There are many sorting algorithms, each with its own approach, advantages, and drawbacks. Below is a summary of the most common ones.

## 1. Bubble Sort ü´ß
**Overview**:  
This algorithm repeatedly steps through the list, compares adjacent items, and swaps them if they are in the wrong order. The pass through the list is repeated until the list is sorted.

- **Best-case time complexity**: \(O(n)\) (when the list is already sorted)
- **Worst-case time complexity**: \(O(n^2)\)
- **Space complexity**: \(O(1)\)

**Pros**: Simple to understand and implement.  
**Cons**: Inefficient for large datasets.

---

## 2. Selection Sort üëâ
**Overview**:  
This algorithm works by selecting the smallest (or largest, depending on sorting order) element from the unsorted portion of the list and swapping it with the first unsorted element.

- **Best-case time complexity**: \(O(n^2)\)
- **Worst-case time complexity**: \(O(n^2)\)
- **Space complexity**: \(O(1)\)

**Pros**: Simple and doesn‚Äôt require additional memory.  
**Cons**: Inefficient for large lists.

---

## 3. Insertion Sort üì©
**Overview**:  
It builds the sorted list one item at a time. It takes each element from the unsorted portion of the list and inserts it into the correct position within the sorted portion.

- **Best-case time complexity**: \(O(n)\) (when the list is already sorted)
- **Worst-case time complexity**: \(O(n^2)\)
- **Space complexity**: \(O(1)\)

**Pros**: Efficient for small datasets or nearly sorted lists.  
**Cons**: Inefficient for large datasets.

---

## 4. Merge Sort ü§ù
**Overview**:  
This is a divide-and-conquer algorithm. It divides the list into two halves, sorts each half recursively, and then merges the sorted halves.

- **Best-case time complexity**: \(O(n \log n)\)
- **Worst-case time complexity**: \(O(n \log n)\)
- **Space complexity**: \(O(n)\) (due to the need for auxiliary arrays)

**Pros**: Stable and efficient for large datasets.  
**Cons**: Requires extra space for the temporary arrays.

---

## 5. Quick Sort üöÄ
**Overview**:  
This algorithm picks an element (called a pivot) from the list and partitions the other elements into two sub-arrays: one with elements smaller than the pivot and one with elements larger. The sub-arrays are then sorted recursively.

- **Best-case time complexity**: \(O(n \log n)\)
- **Worst-case time complexity**: \(O(n^2)\) (when the pivot choices are bad, e.g., sorted or reverse-sorted input)
- **Space complexity**: \(O(\log n)\)

**Pros**: Very efficient in practice for large datasets.  
**Cons**: Worst-case time complexity can be bad without proper pivot selection.

---

## 6. Heap Sort üìö
**Overview**:  
This algorithm uses a binary heap data structure. It first builds a max heap (or min heap) from the input list, then repeatedly extracts the maximum (or minimum) element from the heap and places it in the sorted list.

- **Best-case time complexity**: \(O(n \log n)\)
- **Worst-case time complexity**: \(O(n \log n)\)
- **Space complexity**: \(O(1)\)

**Pros**: Good worst-case performance, no extra space required.  
**Cons**: Not a stable sort and can be slower than quicksort in practice due to more constant factors.

---

## 7. Radix Sort üßÆ
**Overview**:  
A non-comparative sorting algorithm. It sorts numbers digit by digit starting from the least significant digit (LSD) or most significant digit (MSD).

- **Best-case time complexity**: \(O(nk)\), where \(k\) is the number of digits
- **Worst-case time complexity**: \(O(nk)\)
- **Space complexity**: \(O(n + k)\)

**Pros**: Can be very efficient for large datasets where the numbers have a small range of digits.  
**Cons**: Works only with numbers or strings, and not comparisons between arbitrary objects.

---

## 8. Counting Sort üî¢
**Overview**:  
This algorithm assumes that the input consists of integers within a small range. It counts the number of occurrences of each distinct element and uses this information to place the elements in sorted order.

- **Best-case time complexity**: \(O(n + k)\), where \(k\) is the range of input
- **Worst-case time complexity**: \(O(n + k)\)
- **Space complexity**: \(O(k)\)

**Pros**: Very fast when the range of the numbers is not large.  
**Cons**: Not suitable for floating-point numbers or large ranges.

---

## 9. Bucket Sort ü™£
**Overview**:  
This algorithm divides the elements into different "buckets" and then sorts each bucket individually, often using another sorting algorithm like insertion sort. The sorted buckets are then combined.

- **Best-case time complexity**: \(O(n + k)\), where \(k\) is the number of buckets
- **Worst-case time complexity**: \(O(n^2)\), if the elements are not uniformly distributed across buckets
- **Space complexity**: \(O(n + k)\)

**Pros**: Works well when the input is uniformly distributed.  
**Cons**: Not efficient if the data distribution is skewed.

---

## 10. Shell Sort üêö
**Overview**:  
Shell sort generalizes insertion sort to allow the exchange of items that are far apart. The idea is to arrange the list so that, starting anywhere, taking every \(k\)-th element produces a sorted list.

- **Best-case time complexity**: \(O(n \log n)\)
- **Worst-case time complexity**: \(O(n^2)\)
- **Space complexity**: \(O(1)\)

**Pros**: Faster than bubble and insertion sort in practice.  
**Cons**: Its time complexity is highly dependent on the gap sequence used.

---

## 11. Tim Sort üß∏
**Overview**:  
A hybrid sorting algorithm derived from merge sort and insertion sort. It is the default sorting algorithm in Python (`sorted()` function).

- **Best-case time complexity**: \(O(n)\)
- **Worst-case time complexity**: \(O(n \log n)\)
- **Space complexity**: \(O(n)\)

**Pros**: Very efficient in practice, especially for real-world data.  
**Cons**: Slightly more complex to implement.

---

## Conclusion
- **Simple algorithms** (like **Bubble Sort**, **Selection Sort**, **Insertion Sort**) are easy to implement but are inefficient for large datasets (quadratic time complexity).
- **Efficient algorithms** (like **Merge Sort**, **Quick Sort**, **Heap Sort**) are typically faster for large datasets with **\(O(n \log n)\)** time complexity.
- **Non-comparative algorithms** (like **Radix Sort**, **Counting Sort**) can achieve linear time complexity under certain conditions (e.g., when the input is constrained or numeric).
- **Hybrid algorithms** like **Tim Sort** are highly optimized and often preferred for practical use cases.
